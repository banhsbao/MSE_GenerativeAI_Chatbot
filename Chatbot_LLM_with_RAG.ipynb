{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banhsbao/mse_python_chatbox/blob/main/Chatbot_LLM_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxzUBefmbFt7"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get clean\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYqWTtXkYnJf"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq openai==0.27.4 --progress-bar off\n",
        "!pip install -Uqqq watermark==2.3.1 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.173 --progress-bar off\n",
        "!pip install -qqq chromadb==0.3.23 --progress-bar off\n",
        "!pip install -qqq pypdf==3.8.1 --progress-bar off\n",
        "!pip install -qqq pygpt4all==1.1.0 --progress-bar off\n",
        "!pip install -qqq pdf2image==1.16.3 --progress-bar off\n",
        "!pip install -Uqqq tiktoken==0.3.3 --progress-bar off\n",
        "!pip install -Uqqq unstructured[local-inference]==0.5.12 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4GwbgWWVBva"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import textwrap\n",
        "\n",
        "import chromadb\n",
        "import langchain\n",
        "import openai\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader, UnstructuredPDFLoader, YoutubeLoader, PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import GPT4All\n",
        "from pdf2image import convert_from_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8-np6GXaIxJ"
      },
      "source": [
        "Download file pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb1i2YzQVwWL",
        "outputId": "ff392429-e3b5-4f03-ec3d-2f7cf38a0893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16kk1KEqoorpVghSlMLI7O7PCfhr0Fucc\n",
            "To: /content/Li_thuyet_Hadoop.pdf\n",
            "\r  0% 0.00/392k [00:00<?, ?B/s]\r100% 392k/392k [00:00<00:00, 120MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download file pdf\n",
        "!gdown https://drive.google.com/uc?id=16kk1KEqoorpVghSlMLI7O7PCfhr0Fucc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc7j907KVc6q"
      },
      "source": [
        "Load Data & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siK0SmAbVntu",
        "outputId": "ab18ab59-3b6f-4cd1-bbe1-0e180225193b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "OPENAI_API_KEY = getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v-8PNtoZQcC",
        "outputId": "8dc7070f-808a-4d85-8040-6b8c4ef0ea7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:696: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boz6Sb0iaXRM",
        "outputId": "c7b10946-aea6-4b7a-a09a-88e4105fd700"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images = convert_from_path(\"Li_thuyet_Hadoop.pdf\", dpi=88)\n",
        "len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWZYtGg-bShf"
      },
      "outputs": [],
      "source": [
        "images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol9b3mjifXly"
      },
      "source": [
        "Use UnstructuredPDFLoader to load PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3_LCvUdfZwt",
        "outputId": "82c19689-681c-429b-a7b5-7f234536b94e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:unstructured:detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
          ]
        }
      ],
      "source": [
        "# Use UnstructuredPDFLoader to load PDFs from the Internets\n",
        "pdf_loader = UnstructuredPDFLoader(\"Li_thuyet_Hadoop.pdf\")\n",
        "pdf_pages = pdf_loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33IaK0CNfp7l",
        "outputId": "f7ddeb50-8972-4fa0-c5b1-c8906064dd3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text Splitters\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "texts = text_splitter.split_documents(pdf_pages)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5aHhSxPfzyE"
      },
      "source": [
        "Create Embeddings & Vectorstores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kzLeGk6f7YV"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GULkMbE4gEzV",
        "outputId": "10a55ebb-5d57-4d68-8eff-730ad4339a98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(texts, hf_embeddings, persist_directory=\"db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s12dgxmGgTDn"
      },
      "source": [
        "#Use a Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAaRHhvGiFgq"
      },
      "outputs": [],
      "source": [
        "custom_prompt_template = \"\"\"Sử dụng các thông tin sau đây để trả lời câu hỏi của người dùng.\n",
        "Nếu bạn không biết câu trả lời, chỉ cần nói rằng bạn không biết, đừng cố bịa ra câu trả lời.\n",
        "Tất cả câu trả lời của bạn đều phải trả lời bằng tiếng việt\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRr-u4owi5ry"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "def set_custom_prompt():\n",
        "    \"\"\"\n",
        "    Prompt template for QA retrieval for each vectorstore\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=custom_prompt_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot0MuzyCgSYX"
      },
      "outputs": [],
      "source": [
        "prompt = set_custom_prompt()\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=model,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    chain_type_kwargs={'prompt': prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6GdA7S3gb1-"
      },
      "source": [
        "#QA Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boSFtUb7gr0A"
      },
      "outputs": [],
      "source": [
        "def print_response(response: str):\n",
        "    print(\"\\n\".join(textwrap.wrap(response, width=100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgQdLCMWkKe9",
        "outputId": "cac817db-e39a-4e0e-ceb6-a46992fcabb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Có, phạm vi áp dụng nằm trong mục đích của quy trình phối hợp tiếp nhận triển khai yêu cầu xây\n",
            "dựng/nâng cấp bài toán AI của Tổng Công ty Cổ phần Công trình Viettel (VCC).\n"
          ]
        }
      ],
      "source": [
        "query = \"Phạm vi áp dụng có nằm trong mục đích hay không?\"\n",
        "response = chain.run(query)\n",
        "print_response(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sjDD5sZgeoA",
        "outputId": "e0dfbf4e-d548-437e-9c1e-75af0862b414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MapReduce là một mô hình lập trình và kỹ thuật xử lý dữ liệu phân tán trong Big Data. Nó bao gồm hai\n",
            "tác vụ quan trọng là map và reduce. Mô hình này được phát triển bởi Google và được sử dụng để tự\n",
            "động song song hóa và phân phối các phép tính quy mô lớn trên các cụm máy tính lớn.\n"
          ]
        }
      ],
      "source": [
        "query = \"MapReduce là gì?\"\n",
        "response = chain.run(query)\n",
        "print_response(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkgh9_N9jhyD",
        "outputId": "68e00ef3-609c-461f-9998-46b1958fb7ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hadoop là một hệ sinh thái phần mềm mã nguồn mở được sử dụng để xử lý và lưu trữ dữ liệu lớn. Nó bao\n",
            "gồm các thành phần như HDFS, Hive, Pig, YARN, MapReduce, Spark, HBase, Oozie, Sqoop, Zookeeper, và\n",
            "nhiều công cụ khác. Hadoop được xây dựng trên ngôn ngữ Java và tương thích trên nhiều nền tảng khác\n",
            "nhau.\n"
          ]
        }
      ],
      "source": [
        "query = \"Hadoop là gì?\"\n",
        "response = chain.run(query)\n",
        "print_response(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_89910yjv0x",
        "outputId": "7dac4224-c5b9-4d1b-de87-d01356801b09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kiến trúc của Hadoop bao gồm các thành phần sau: 1. Hadoop Distributed File System (HDFS): Hệ thống\n",
            "file phân tán của Hadoop, nằm ở trên cùng của hệ thống file cục bộ và giám sát quá trình. 2.\n",
            "MapReduce: Mô hình xử lý dữ liệu phân tán trong Hadoop, sử dụng các công cụ để phân tách và xử lý dữ\n",
            "liệu trên các nút trong cụm. 3. YARN (Yet Another Resource Negotiator): Quản lý các thành viên trong\n",
            "nhóm máy chủ và phân phối tài nguyên cho các ứng dụng chạy trên Hadoop. 4. ZooKeeper: Hệ thống quản\n",
            "lý và bầu cử leader trong Hadoop, đảm bảo tính nhất quán và sẵn sàng của dữ liệu. 5. Hadoop Common:\n",
            "Bao gồm các thư viện và công cụ chung được sử dụng bởi các thành phần khác trong Hadoop. 6. Hadoop\n",
            "Ozone: Hệ thống lưu trữ đối tượng phân tán trong Hadoop, cung cấp khả năng lưu trữ và truy xuất dữ\n",
            "liệu theo kiểu đối tượng. 7. Hadoop HDFS Federation: Cung cấp khả năng mở rộng và phân chia dữ liệu\n",
            "trên nhiều cụm HDFS. 8. Hadoop HDFS High Availability: Cung cấp khả năng sao lưu và khôi phục tự\n",
            "động cho HDFS, đảm bảo tính sẵn sàng và tin cậy của dữ liệu. 9. Hadoop HDFS Erasure Coding: Sử dụng\n",
            "mã hóa xóa để giảm bớt lưu trữ dự phòng và tăng hiệu suất lưu trữ trong HDFS. 10. Hadoop HDFS\n",
            "Snapshots: Cung cấp khả năng tạo và quản lý các bản snapshot của dữ liệu trong HDFS, cho phép khôi\n",
            "phục dữ liệu về trạng thái trước đó.\n"
          ]
        }
      ],
      "source": [
        "query = \"Kiến trúc của Hadoop gồm những gì?\"\n",
        "response = chain.run(query)\n",
        "print_response(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
